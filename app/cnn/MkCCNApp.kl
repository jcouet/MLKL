//************************************************************************************************//
//                                                                                                //
//  Code part of the project MLKL                                                                 //
//                                                                                                //
//  couet.julien@gmail.com                                                                        //
//                                                                                                //
//************************************************************************************************//

require MLKL; 


operator entry() {

  String path_config = "C:/Users/Julien/Documents/Dev/MLKL/app/cnn/cnn_config.mlkl";
  String path_image = 'C:/Users/Julien/Documents/Dev/MLKL/resources/mnist/4.bmp';

  // Load the configuration file and set the network layers
  MkCNNConfig config;
  MkCNNLayer layers[];
  if(!config.config(path_config, layers))
    return;

  // Create the network
  MkCNNNetwork nn(config, layers);

  // Set the initiale learning rate
  nn.optimizer().learningRate(nn.optimizer().learningRate()* sqrt(config.batch_size));
  nn.display();

  // Declare the enumeration, here we can set the larning rate decay
  // However, it should be set from the file, see Convnet for this
  MkCNNEnumEpoch epoch_enum(); 
  
  // Load the training data
  MkCNNData data = LoadTrainingCIFAR(config, true, -1.0, -1.0);
  //data.loadTrainingMNIST(config); 
 
  // Train the network
  // The network (the layers' weights) is tested and saved at each epch
  nn.train(config, data, epoch_enum);


  // Finally test it and save it
  Float64 image_data[][] = LoadValidationCIFAR(config, true, -1.0, -1.0);
  report(nn.predictRescale(image_data));
}

