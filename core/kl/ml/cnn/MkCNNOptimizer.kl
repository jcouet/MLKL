/**************************************************************************************************/
/*                                                                                                */
/*  Informations :                                                                                */
/*      This code is part of the project MLKL                                                     */
/*                                                                                                */
/*  Contacts :                                                                                    */
/*      couet.julien@gmail.com                                                                    */
/*                                                                                                */
/**************************************************************************************************/

require MLKL; 
 

/**************************************************************************************************/
/*                                             2. ASM Batches                                     */
const Index MK_OPTIMIZER_GD = 0;
const Index MK_OPTIMIZER_GDLM = 1;
 
interface MkCNNOptimizerInterface {
  update!(Float64 dw[], Float64 H[], io Float64 W[]);
  Boolean requiresHessian();
  learningRate!(Float64 learning_rate);
  weigthDecay!(Float64 weigth_decay);
  Float64 learningRate();
  Float64 weigthDecay();
  display();
  reset!();
};

object MkCNNOptimizerBase : MkCNNOptimizerInterface {  
  protected Float64 learning_rate;  // Learning rate
  protected Float64 weigth_decay;   // Weight decay
};

public MkCNNOptimizerBase.display() {
  report("learning_rate " + this.learning_rate);
  report("weigthDecay " + this.weigth_decay);
}

public MkCNNOptimizerBase.update!(
  Float64 dw[], 
  Float64 H[], 
  io Float64 w[]) 
{}

public Boolean MkCNNOptimizerBase.requiresHessian() {
  return false;
}

protected MkCNNOptimizerBase.init!(Float64 learning_rate, Float64 weigth_decay) {
  this.learning_rate = learning_rate;
  this.weigth_decay = weigth_decay;
}

public MkCNNOptimizerBase.learningRate!(Float64 learning_rate) {
  this.learning_rate = learning_rate;
}

public MkCNNOptimizerBase.weigthDecay!(Float64 weigth_decay) {
  this.weigth_decay = weigth_decay;
}

public Float64 MkCNNOptimizerBase.learningRate() {
  return this.learning_rate;
}

public Float64 MkCNNOptimizerBase.weigthDecay() {
  return this.weigth_decay;
}

public MkCNNOptimizerBase.reset!() {
}
/*                                       end Constructor/Destructor                               */
/**************************************************************************************************/

                                          /***********************/

/**************************************************************************************************/
/*                                             2. ASM Batches                                     */
object MkCNNOptimizerGD : MkCNNOptimizerBase {};

public MkCNNOptimizerGD() {
  this.init(0.01, 0.02);
}
 
public MkCNNOptimizerGD(Float64 learning_rate, Float64 weigth_decay) {
  this.init(learning_rate, weigth_decay);
}

operator MkCNNOptimizerGDUpdate_task<<<i>>>(
  MkCNNOptimizerGD gdlm,
  Float64 dw[], 
  io Float64 w[]) 
{
  w[i] -= (gdlm.learningRate() / (w[i] + gdlm.weigthDecay())) * (dw[i]); 
}

public MkCNNOptimizerGD.update!(
  Float64 dw[], 
  Float64 H[], 
  io Float64 w[]) 
{
  MkCNNOptimizerGDUpdate_task<<<w.size()>>>(this, dw, w);
}

public Boolean MkCNNOptimizerGD.requiresHessian() {
  return false;
}
/*                                       end Constructor/Destructor                               */
/**************************************************************************************************/

                                          /***********************/

/**************************************************************************************************/
/*                                             2. ASM Batches                                     */
object MkCNNOptimizerGDLM : MkCNNOptimizerBase {};

public MkCNNOptimizerGDLM() {
  this.init(0.00085, 0.02);
}
 
public MkCNNOptimizerGDLM(Float64 learning_rate, Float64 weigth_decay) {
  this.init(learning_rate, weigth_decay);
}

operator MkCNNOptimizerGDLMUpdate_task<<<i>>>(
  Ref<MkCNNOptimizerGDLM> gdlm,
  Float64 dw[], 
  Float64 H[], 
  io Float64 w[]) 
{
  w[i] = w[i] - (gdlm.learningRate() / (H[i] + gdlm.weigthDecay())) * (dw[i]); 
}

public MkCNNOptimizerGDLM.update!(
  Float64 dw[], 
  Float64 H[], 
  io Float64 w[]) 
{
  MkCNNOptimizerGDLMUpdate_task<<<w.size()>>>(this, dw, H, w);
}

public MkCNNOptimizerGDLM.display() {
  //report("learning_rate GDLM " + this.learning_rate);
  //report("weigthDecay GDLM " + this.weigth_decay);
}

public Boolean MkCNNOptimizerGDLM.requiresHessian() {
  return true;
}
/*                                       end Constructor/Destructor                               */
/**************************************************************************************************/

                                          /***********************/

/**************************************************************************************************/
/*                                             2. ASM Batches                                     */
// http://xcorr.net/2014/01/23/adagrad-eliminating-learning-rates-in-stochastic-gradient-descent/
/*
object MkCNNOptimizerADAGRAD : MkCNNOptimizerInterface {  
  Float64 learning_rate;      // learning rate
  Float64 E[MkHashTable][];
};

public MkCNNOptimizerADAGRAD() {
  
  this.learning_rate = 0.01;
}
 
public MkCNNOptimizerADAGRAD(Float64 learning_rate) {
  
  this.learning_rate = learning_rate;
}

operator MkCNNOptimizerADAGRADUpdate_task<<<i>>>(
  io MkCNNOptimizerADAGRAD gdlm,
  Float64 dw[], 
  io Float64 w[]) 
{
  gdlm.E[i] += dw[i] * dw[i];
  //w[i] -= gdlm.learning_rate * dw[i] / sqrt(gdlm.E[i]);
}

public MkCNNOptimizerADAGRAD.update(
  Float64 dw[], 
  io Float64 w[]) 
{
  if(!E.has(HashTable(w)));
  {

  }
  if(this.E[w].empty())
    this.E[w] = ;

  MkCNNOptimizerADAGRADUpdate_task<<<w.sixe()>>>(this, dw, w);
}

public MkCNNOptimizerADAGRAD.reset() {
  
  this.E.clear();
}

public Boolean MkCNNOptimizerADAGRAD.requiresHessian() {

  return false;
}
*/
/*                                       end Constructor/Destructor                               */
/**************************************************************************************************/
 