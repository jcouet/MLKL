//************************************************************************************************//
//                                                                                                //
//  Code part of the project MLKL                                                                 //
//                                                                                                //
//  couet.julien@gmail.com                                                                        //
//                                                                                                //
//************************************************************************************************//

require MLKL; 
 

//************************************************************************************************//
//                                               Optimizer                                        //
const Index MK_OPTIMIZER_GD = 0;
const Index MK_OPTIMIZER_GDLM = 1;
const Index MK_OPTIMIZER_AGD = 2;
const Index MK_OPTIMIZER_RMSP = 3;
const Index MK_OPTIMIZER_MGD = 4;

function String MkCNNOptimizerModeAsStr(Index mode) {
  if(mode == MK_OPTIMIZER_GD) return "MK_OPTIMIZER_GD";    
  else if(mode == MK_OPTIMIZER_GDLM) return "MK_OPTIMIZER_GDLM";
  else if(mode == MK_OPTIMIZER_AGD) return "MK_OPTIMIZER_AGD";
  else if(mode == MK_OPTIMIZER_RMSP) return "MK_OPTIMIZER_RMSP";
  else if(mode == MK_OPTIMIZER_MGD) return "MK_OPTIMIZER_MGD";
  else return "MK_OPTIMIZER_UNKNOWN";
}

/// Interface for optimizer
interface MkCNNOptimizer {
  Index mode();
  String modeAsStr();
  update!(UInt64 layer_uid, Float64 dw[], Float64 h[], io Float64 w[]);
  Boolean requiresHessian();
  learningRate!(Float64 learning_rate);
  weigthDecay!(Float64 weigth_decay);
  Float64 learningRate();
  Float64 weigthDecay();
  display();
  reset!();
};

/// Base class for optimizer
object MkCNNOptimizerBase : MkCNNOptimizer {  
  protected Index mode;
  Float64 learning_rate;  // Learning rate
  Float64 weigth_decay;   // Weight decay
};

/// Init the optimizer value
protected MkCNNOptimizerBase.init!(Float64 learning_rate, Float64 weigth_decay) {
  this.learning_rate = learning_rate;
  this.weigth_decay = weigth_decay;
}

/// Return the current mode, either MK_LOSS_CE or MK_LOSS_MSE
public Index MkCNNOptimizerBase.mode() {
  return this.mode;
}

/// Return the current mode as string
public String MkCNNOptimizerBase.modeAsStr() {
  return MkCNNOptimizerModeAsStr(this.mode);
}

/// Display the optimizer attributs
public MkCNNOptimizerBase.display() {
  report("optimizer        : " + this.modeAsStr());
  report("learning_rate    : " + this.learning_rate);
  report("weigthDecay      : " + this.weigth_decay);
}

/// Set the learning rate
public MkCNNOptimizerBase.learningRate!(Float64 learning_rate) {
  this.learning_rate = learning_rate;
}

/// Get the learning rate
public Float64 MkCNNOptimizerBase.learningRate() {
  return this.learning_rate;
}

/// Set the weigth decay
public MkCNNOptimizerBase.weigthDecay!(Float64 weigth_decay) {
  this.weigth_decay = weigth_decay;
}

/// Get the weigth decay
public Float64 MkCNNOptimizerBase.weigthDecay() {
  return this.weigth_decay;
}

/// Reset
public MkCNNOptimizerBase.reset!() {}

/// Return TRUE if the optimizer needs an Hessian matrix
public Boolean MkCNNOptimizerBase.requiresHessian() {
  return false;
}

/// Update the optimizer
/// To-be overload
public MkCNNOptimizerBase.update!(
  UInt64 layer_uid,
  Float64 dw[], 
  Float64 h[], 
  io Float64 w[]) 
{
}
//                                               Optimizer                                        //
//************************************************************************************************//

                                          //*********************//

//************************************************************************************************//
//                                            Gradient Descente                                   //
/// Class for gradient descente optimizer
object MkCNNOptimizerGD : MkCNNOptimizerBase {};

/// Constructor
public MkCNNOptimizerGD() {
  this.init(0.01, 0.1);
  this.mode = MK_OPTIMIZER_GD;
}
 
public MkCNNOptimizerGD(Float64 learning_rate, Float64 weigth_decay) {
  this.parent.init(learning_rate, weigth_decay);
  this.mode = MK_OPTIMIZER_GD;
}

operator MkCNNOptimizerGDUpdate_task<<<i>>>(Ref<MkCNNOptimizerGD> opti, Float64 dw[], io Float64 w[]) {
  w[i] = w[i] - opti.learningRate() * (dw[i] + opti.weigthDecay() * w[i]); 
}

public MkCNNOptimizerGD.update!(
  UInt64 layer_uid,
  Float64 dw[], 
  Float64 h[], 
  io Float64 w[]) 
{
  MkCNNOptimizerGDUpdate_task<<<w.size()>>>(this, dw, w);
}
//                                            Gradient Descente                                   //
//************************************************************************************************//

                                          //*********************//

//************************************************************************************************//
//                                    Gradient Descente Levenbarq-Marquant                        //
/// Class for gradient descente Levenbarq-Marquant optimizer
object MkCNNOptimizerGDLM : MkCNNOptimizerBase {};

/// Constructor
public MkCNNOptimizerGDLM() {
  // constant to prevent step size from becoming too large when H is small
  this.init(0.00085, 0.02);
  this.mode = MK_OPTIMIZER_GDLM;
}
 
public MkCNNOptimizerGDLM(Float64 learning_rate, Float64 weigth_decay) {
  this.init(learning_rate, weigth_decay);
  this.mode = MK_OPTIMIZER_GDLM;
}

operator MkCNNOptimizerGDLMUpdate_task<<<i>>>(
  Ref<MkCNNOptimizerGDLM> opti,
  Float64 dw[], 
  Float64 h[], 
  io Float64 w[]) 
{
  w[i] = w[i] - (opti.learningRate() / (h[i] + opti.weigthDecay()) ) * dw[i]; 
}

public MkCNNOptimizerGDLM.update!(
  UInt64 layer_uid,
  Float64 dw[], 
  Float64 h[],   
  io Float64 w[]) 
{
  MkCNNOptimizerGDLMUpdate_task<<<w.size()>>>(this, dw, h, w);
}

public Boolean MkCNNOptimizerGDLM.requiresHessian() {
  return true;
}
//                                    Gradient Descente Levenbarq-Marquant                        //
//************************************************************************************************//

                                          //*********************//

//************************************************************************************************//
//                                           Stateful optimizers                                  //
/// Base class for optimizer
object MkCNNOptimizerStatefulBase : MkCNNOptimizerBase {  
  protected Float64 E[UInt64][];
  Float64 eps;
};

/// Init the optimizer value
protected MkCNNOptimizerStatefulBase.init!(Float64 learning_rate, Float64 weigth_decay) {
  this.parent.init(learning_rate, weigth_decay);
  this.eps = 1.e-8;
}

public Float64[] MkCNNOptimizerStatefulBase.get!(UInt64 layer_uid, Float64 w[]) {
  if(!this.E.has(layer_uid))
  {
    Float64 temp[]; temp.resize(w.size());   
    this.E.set(layer_uid, temp);
  }
  return this.E.get(layer_uid);
}

public MkCNNOptimizerStatefulBase.set!(UInt64 layer_uid, Float64 g[]) {
  if(this.E.has(layer_uid))
    this.E.set(layer_uid, g);
}

public MkCNNOptimizerStatefulBase.reset!() {
  this.E.clear();
}
//                                           Stateful optimizers                                  //
//************************************************************************************************//

                                          //*********************//

//************************************************************************************************//
//                                       Adaptive Gradient Descente                               //
// J. Duchi, E. Hazan and Y. Singer,
// Adaptive subgradient methods for online learning and stochastic optimization
// The Journal of Machine Learning Research, pages 2121-2159, 2011.
// http://xcorr.net/2014/01/23/adagrad-eliminating-learning-rates-in-stochastic-gradient-descent/
object MkCNNOptimizerAGD : MkCNNOptimizerStatefulBase {};

public MkCNNOptimizerAGD() {
  this.parent.init(0.01, 0.0);
  this.mode = MK_OPTIMIZER_AGD;
}
 
public MkCNNOptimizerAGD(Float64 learning_rate) {
  this.parent.init(learning_rate, 0.0);
  this.mode = MK_OPTIMIZER_AGD;
}

operator MkCNNOptimizerAGDUpdate_task<<<i>>>(
  Ref<MkCNNOptimizerAGD> opti,
  Float64 dw[], 
  io Float64 w[],
  io Float64 g[]) 
{
  g[i] += dw[i] * dw[i];
  w[i] = w[i] - opti.learning_rate * dw[i] / (sqrt(g[i]) + opti.eps);
}

public MkCNNOptimizerAGD.update!(
  UInt64 layer_uid,
  Float64 dw[], 
  Float64 h[],   
  io Float64 w[]) 
{
  Float64 g[] = this.get(layer_uid, w);
  MkCNNOptimizerAGDUpdate_task<<<w.size()>>>(this, dw, w, g);
  this.set(layer_uid, g);
}
//                                       Adaptive Gradient Descente                               //
//************************************************************************************************//

                                          //*********************//

//************************************************************************************************//
//                                                RMS Prop                                        //
// T. Tielemanand G.E. Hinton,
// Lecture 6.5 - rmsprop, COURSERA: Neural Networks for Machine Learning (2012)
object MkCNNOptimizerRMSP : MkCNNOptimizerStatefulBase {};

public MkCNNOptimizerRMSP() {
  this.parent.init(0.0001, 0.99);
  this.mode = MK_OPTIMIZER_RMSP;
}
 
public MkCNNOptimizerRMSP(Float64 learning_rate, Float64 weigth_decay) {
  this.parent.init(learning_rate, weigth_decay);
  this.mode = MK_OPTIMIZER_RMSP;
}

operator MkCNNOptimizerRMSPUpdate_task<<<i>>>(
  Ref<MkCNNOptimizerRMSP> opti,
  Float64 dw[], 
  io Float64 w[],
  io Float64 g[]) 
{
  g[i] = opti.weigth_decay * g[i] + (1.0 - opti.weigth_decay) * dw[i] * dw[i];
  w[i] = w[i] - opti.learning_rate * dw[i] / (sqrt(g[i]) + opti.eps);
}

public MkCNNOptimizerRMSP.update!(
  UInt64 layer_uid,
  Float64 dw[], 
  Float64 h[],   
  io Float64 w[]) 
{
  Float64 g[] = this.get(layer_uid, w);
  MkCNNOptimizerRMSPUpdate_task<<<w.size()>>>(this, dw, w, g);
  this.set(layer_uid, g);
}
//                                                RMS Prop                                        //
//************************************************************************************************//

                                          //*********************//

//************************************************************************************************//
//                                       Adaptive Gradient Descente                               //
// SGD with Momentum, B T Polyak,
// Some methods of speeding up the convergence of iteration methods
// USSR Computational Mathematics and Mathematical Physics, 4(5):1-17, 1964.
object MkCNNOptimizerMGD : MkCNNOptimizerStatefulBase {
  Float64 mu; // Momentum
};

public MkCNNOptimizerMGD() {
  this.parent.init(0.01, 0.0);
  this.mu = 0.9;
  this.mode = MK_OPTIMIZER_MGD;
}
 
public MkCNNOptimizerMGD(Float64 learning_rate, Float64 weigth_decay) {
  this.parent.init(learning_rate, weigth_decay);
  this.mu = 0.9;
  this.mode = MK_OPTIMIZER_MGD;
}

operator MkCNNOptimizerMGDUpdate_task<<<i>>>(
  Ref<MkCNNOptimizerMGD> opti,
  Float64 dw[], 
  io Float64 w[],
  io Float64 g[]) 
{
  g[i] = opti.mu * g[i] - opti.learning_rate * (dw[i] + w[i] * opti.weigth_decay);
  w[i] += g[i];
}

public MkCNNOptimizerMGD.update!(
  UInt64 layer_uid,
  Float64 dw[], 
  Float64 h[],   
  io Float64 w[]) 
{
  Float64 g[] = this.get(layer_uid, w);
  MkCNNOptimizerMGDUpdate_task<<<w.size()>>>(this, dw, w, g);
  this.set(layer_uid, g);
}
//                                       Adaptive Gradient Descente                               //
//************************************************************************************************//