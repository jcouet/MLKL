//************************************************************************************************//
//                                                                                                //
//  Code part of the project MLKL                                                                 //
//                                                                                                //
//  couet.julien@gmail.com                                                                        //
//                                                                                                //
//************************************************************************************************//

require MLKL; 

/**
  The AlembicArchiveReader is a wrapper for the AlembicIArchive. 
  It provides access to the higher level reader objects such as the AlembicXformReader.
  \example

  require MLKL;

  operator entry() {
    
  }

  \endexample
*/
 
const Index MK_GRAD_CHECK_ALL = 0;
const Index MK_GRAD_CHECK_FIRST = 1;
const Index MK_GRAD_CHECK_RANDOM = 2;


/// Class for Convolution Neural-Network 
object MkCNNNetwork {
  private String name;
  private MkCNNDefs defs;
  private MkCNNOptimizer optimizer;
  private MkCNNLoss loss_function;
  private MkCNNLayers layers;
};

/// Initilisation, called by the contructeurs and derived classes
private MkCNNNetwork.init!(Index loss_function, Index optimizer, String name) {
  this.name = name;
  this.layers = MkCNNLayers();

  switch(loss_function)
  {
    case MK_LOSS_MSE:
      this.loss_function = MkCNNLossMSE();
    break;
    
    case MK_LOSS_CE:
      this.loss_function = MkCNNLossCE();
    break;

    default :
      this.loss_function = MkCNNLossMSE();
    break;
  }

  switch(optimizer)
  {
    case MK_OPTIMIZER_GD:
      this.optimizer = MkCNNOptimizerGD();
    break;
    
    case MK_OPTIMIZER_GDLM:
      this.optimizer = MkCNNOptimizerGDLM();
    break;

    default :
      this.optimizer = MkCNNOptimizerGD();
    break;
  }
}

/// Constructor
public MkCNNNetwork(Index loss_function, Index optimizer, String name) {
  this.init(loss_function, optimizer, name);
}

/// Constructor
public MkCNNNetwork(Index loss_function, Index optimizer) {
  this.init(loss_function, optimizer, "");
}

/// Constructor
public MkCNNNetwork(Index loss_function, Index optimizer, String name, io MkCNNLayer layers[]) {
  this.init(loss_function, optimizer, name);
  this.add(layers);
}

/// Constructor
public MkCNNNetwork(Index loss_function, Index optimizer, io MkCNNLayer layers[]) {
  this.init(loss_function, optimizer, "");
  this.add(layers);
}

/// Display the netwok attributs
public MkCNNNetwork.display() {
  report("\n\n\n-------------------- Network --------------------");
  report("Attributs");
  report("name             : " + this.name);
  report("gpu              : " + this.defs.gpu());
  report("taskSize         : " + this.defs.taskSize());
  report("loss_function    : " + this.loss_function.modeAsStr());
  this.optimizer().display();  
  this.layers.display();
}

/// Return the input dimension of whole networks
public Index MkCNNNetwork.inDim() { 
  return this.layers.head().inSize(); 
}

/// Return the output dimension of whole networks
public Index MkCNNNetwork.outDim() { 
  return this.layers.tail().outSize(); 
}

/// Return the netork name
public String MkCNNNetwork.name() { 
  return this.name;
}

/// Return a pointer to the netork optimizer
public Ref<MkCNNOptimizer> MkCNNNetwork.optimizer() {
  return this.optimizer;
}

/// Add a new layer to the network
public MkCNNNetwork.add!(io MkCNNLayer layer) {
  this.layers.add(layer);
}

/// Add a stack pf new layers to the network
public MkCNNNetwork.add!(io MkCNNLayer layers[]) {
  for(Index i=0; i<layers.size(); ++i)
    this.layers.add(layers[i]);
}

/// Train the network
public MkCNNNetwork.train!(MkCNNData data, io MkCNNConfig config, io MkCNNEnumEpoch epoch_enum) {
    
  Index batch_size = config.batch_size;
  Index nb_epoch = config.epoch;

  // Initiialization of the layers weights
  this.optimizer.reset();
  this.layers.initWeight();
   
  // If there a saved network weights file, load the layers weigths
  if(config.load_file_path.length())
  {
    if(!config.load(this.layers))
      return;
  }

  MkCNNEnumBatch enum_batch(data.train_images.size(), batch_size); 
  
  // Start training
  for (Index i=0; i<nb_epoch; i++) 
  {
    enum_batch.reset(i, nb_epoch);
    
    if(this.optimizer.requiresHessian())
      this.calcHessian(data.train_images, 500);

    for (Index j=0; j<data.train_images.size(); j+=batch_size) 
    {
      this.trainOnce(j, data.train_images, data.train_labels, Math_min(batch_size, data.train_images.size() - j));
      enum_batch.update();
    }

    // Update the display and test the network
    epoch_enum.update(this, data.test_images, data.test_labels);
    
    // If we save the network at each epoch, write over the same file
    if(config.auto_saving && config.save)
      config.save(this.layers);
  }

  if(config.save)
    config.save(this.layers);
}

/// Return the prediction of the input
public Float64[] MkCNNNetwork.predict!(Float64 ins[]) {
  return this.fprop(ins, 0);
}

inline Float64 RescaleTemp(Float64 x) {
  return 100.0 * (x - (-0.8)) / (+0.8 - (-0.8));
}

/// Return the prediction of the input
public Float64[] MkCNNNetwork.predictRescale!(Float64 ins[]) {
  Float64 outs[] = this.fprop(ins, 0);
  for(Index i=0; i<outs.size(); ++i)
    outs[i] = RescaleTemp(outs[i]);
  return outs;
}

/// Test the network, use after training 
public MkCNNNetworkResult MkCNNNetwork.test!(Float64 ins[][], Index t[]) {
  
  MkCNNNetworkResult test_result;
  for (Index i = 0; i < ins.size(); i++) 
  {
    Index predicted = MaxIndex(this.predict(ins[i]));
    if (predicted == t[i]) 
      test_result.num_success++;
    test_result.num_total++;
    //test_result.confusion_matrix[predicted][actual]+=1.0;
  }
  return test_result;
}

/// To Remove == set within the function where it's called
private Float64 MkCNNNetwork.targetValueMin() { 
  return Float64(this.layers.tail().neuron().scale().x); 
}

/// To Remove == set within the function where it's called
private Float64 MkCNNNetwork.targetValueMax() { 
  return Float64(this.layers.tail().neuron().scale().y); 
}

/// Convert 1D label array to 2D (image) array
private MkCNNNetwork.label2Vector(Index batch_index, Index size, Index t[], io Float64 vec[][]) {
  Index out_dim = this.outDim();
  if(size <= 0 || out_dim <= 0) return; 
  
  Float64 temp[]; temp.resize(out_dim);
  for (Index i = 0; i < out_dim; i++) 
    temp[i] = this.targetValueMin();

  vec.resize(size);
  for (Index i = 0; i < size; i++) 
  {
    if(t[batch_index + i] >= out_dim) 
      return;
    vec[i] = temp.clone();
    vec[i][t[batch_index + i]] = this.targetValueMax();
  }
}

/// Train one batch
private MkCNNNetwork.trainOnce!(Index batch_index, Float64 ins[][], Float64 t[][], Index size) {
  Ref<MkCNNOptimizer> opti = this.optimizer;
  if(size == 1) 
  {
    Float64 outs[] = this.fprop(ins[0], 0);
    this.bprop(outs, t[0], 0);
    this.layers.updateWeights(opti, 1, 1);
  } 

  else 
  {
    Index num_tasks = size < this.defs.taskSize() ? 1 : this.defs.taskSize();
    Index data_per_thread = size / num_tasks;
    Index remaining = size;

    for (Index i = 0; i < num_tasks; i++) 
    {
      Index num = (i == (num_tasks - 1)) ? remaining : data_per_thread;
      for (Index j = 0; j < num; j++) 
      {
        Float64 outs[] = this.fprop(ins[batch_index + i*num + j], i); 
        this.bprop(outs, t[i*num + j], i);
      }
      remaining -= num;
    }

    this.layers.updateWeights(opti, num_tasks, size);
  }
}  

/// Overload, Train one batch with 1D label array
private MkCNNNetwork.trainOnce!(Index batch_index, Float64 ins[][], Index t[], Index size) {
  Float64 v[][];
  this.label2Vector(batch_index, size, t, v);
  this.trainOnce(batch_index, ins, v, size);
} 

/// Check if a link is canonical
private Boolean MkCNNNetwork.isCanonicalLink(Ref<MkCNNNeuron> h, Ref<MkCNNLoss> e) {  
  if (h.mode() == MK_NEURON_SIG && e.mode() == MK_LOSS_CE)  return true;
  if (h.mode() == MK_NEURON_TANH && e.mode() == MK_LOSS_CE) return true;
  if (h.mode() == MK_NEURON_ID && e.mode() == MK_LOSS_MSE)  return true;
  return false;
}

/// Computation of the hessian
private MkCNNNetwork.calcHessian!(Float64 ins[][], Index size_init_hessian) {
  Index size = Math_min(ins.size(), size_init_hessian);
  for(Index i=0; i<size; i++) 
    this.bprop2nd(this.fprop(ins[i], 0));
  this.layers.divideHessian(size);
}

private Float64 MkCNNNetwork.getLoss(Float64 outs[], Float64 t[]) {
  Float64 e = 0.0;
  for (Index i = 0; i < outs.size(); i++)
    e += Float64(this.loss_function.f(outs[i], t[i]));
  return e;
}

/// Forward propagation
private Float64[] MkCNNNetwork.fprop!(Float64 ins[], Index idx) {
  if (ins.size() != this.inDim()) return ins;
  return this.layers.head().fprop(ins, idx);
}

/// Backward propagetion 
private MkCNNNetwork.bprop!(Float64 outs[], Float64 t[], Index idx) {
  Float64 delta[]; delta.resize(this.outDim());
  
  Ref<MkCNNNeuron> h = this.layers.tail().neuron();
  if (this.isCanonicalLink(h, this.loss_function)) 
  {
    for (Index i = 0; i < this.outDim(); i++)
      delta[i] = outs[i] - t[i];  
  } 
  else 
  {
    for (Index i = 0; i < this.outDim(); i++)
      delta[i] = this.loss_function.df(outs[i], t[i]) * h.df(outs[i]);
  }
 
  this.layers.tail().bprop(delta, idx);
}

/// 2nd Backward propagation, use for Hessian computation 
private MkCNNNetwork.bprop2nd!(Float64 outs[]) {
  Float64 delta[]; delta.resize(this.outDim());
  
  Ref<MkCNNNeuron> h = this.layers.tail().neuron();
  if (this.isCanonicalLink(h, this.loss_function)) 
  {
    for (Index i = 0; i < this.outDim(); i++)
      delta[i] = this.targetValueMax() * h.df(outs[i]);  
  } 
  else 
  {
    for (Index i = 0; i < this.outDim(); i++)
      delta[i] = this.targetValueMax() * h.df(outs[i]) * h.df(outs[i]); // FIXME
  }

  this.layers.tail().bprop2nd(delta);
}

/*
  private Float64 MkCNNNetwork.calcDeltaDiff!(
    Float64 ins[][], 
    Float64 v[][], 
    Index data_size, 
    io Float64 w[], 
    io Float64 dw[], 
    Index check_index) 
  {
    Float64 delta = 1e-10;

    for(Index i=0; i<dw.size(); ++i)
      dw[i] = 0.0;
   
    // calculate dw/dE by bprop
    for (Index i = 0; i < data_size; i++) 
    {
      Float64 outs[] = this.fprop(ins[i], 0);
      this.bprop(outs, v[i], 0);
    }
    Float64 delta_by_bprop = dw[check_index];

    // calculate dw/dE by numeric
    Float64 prev_w = w[check_index];
    w[check_index] = prev_w + delta;
    Float64 f_p = 0.0;
    for (Index i = 0; i < data_size; i++) 
    {
      Float64 outs[] = this.fprop(ins[i], 0);
      f_p += this.getLoss(outs, v[i]);
    }

    Float64 f_m = 0.0;
    w[check_index] = prev_w - delta;
    for (Index i = 0; i < data_size; i++) 
    {
      Float64 outs[] = this.fprop(ins[i], 0);
      f_m += this.getLoss(outs, v[i]);
    }
    w[check_index] = prev_w;

    Float64 delta_by_numerical = (f_p - f_m) / (2.0 * delta);
    return abs(delta_by_bprop - delta_by_numerical);
  }

  public Boolean MkCNNNetwork.gradientCheck!(
    Float64 ins[][], 
    Index t[], 
    Index data_size, 
    Float64 eps, 
    Index mode) 
  {
    if(this.layers.empty()) return false;

    Float64 v[][];
    this.label2Vector(0, data_size, t, v);

    Ref<MkCNNLayer> current = this.layers.head();
    while((current = current.next()) != null) 
    { 
      // ignore first input layer
      Float64 w[] = current.weight();
      Float64 b[] = current.bias();
      Float64 dw[] = current.weightDiff(0);
      Float64 db[] = current.biasDiff(0);

      if (w.size() == 0) continue;
      
      switch (mode) 
      {
        case MK_GRAD_CHECK_ALL:
          for (Index i = 0; i < w.size(); i++) 
          {
            if (this.calcDeltaDiff(ins, v, data_size, w, dw, i) > eps) 
              return false;
          }
          for (Index i = 0; i < b.size(); i++) 
          {
            if (this.calcDeltaDiff(ins, v, data_size, b, db, i) > eps) 
              return false;
          }
        break;
        
        case MK_GRAD_CHECK_FIRST:
          if (this.calcDeltaDiff(ins, v, data_size, w, dw, 0) > eps) 
            return false;
          if (this.calcDeltaDiff(ins, v, data_size, b, db, 0) > eps) 
           false;
        break;
        
        case MK_GRAD_CHECK_RANDOM:
          for (Index i = 0; i < 10; i++) 
          {
            Index index; UniformRand(0, w.size() - 1, index);
            if (this.calcDeltaDiff(ins, v, data_size, w, dw, index) > eps) 
              return false;
          }
          for (Index i = 0; i < 10; i++) 
          {
              Index index; UniformRand(0, b.size() - 1, index);
              if (this.calcDeltaDiff(ins, v, data_size, b, db, index) > eps) 
                return false;
          }
        break;

        default:
          report("unknown grad-check type");
          //throw nn_error("unknown grad-check type");
        break;
      }
    }

    return true;
  }
*/
